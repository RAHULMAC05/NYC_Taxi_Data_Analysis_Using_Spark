# üöñ NYC Taxi Data Analysis Using Apache Spark

A data engineering and analysis project using **PySpark** on the popular **NYC Taxi Trip dataset (March 2016)**.  
The project demonstrates large-scale data processing, partitioning, aggregation, window functions, UDFs, and correlation analysis ‚Äî all in a distributed environment using **Databricks Notebook (.dbc)**.


---

## üìä Dataset Source

**NYC Taxi Trip Data (March 2016)**  
üîó Download link: [üëâ Click Here to Download (Parquet)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

- Format: `.parquet`
- Columns: `pickup_datetime`, `dropoff_datetime`, `trip_distance`, `fare_amount`, `payment_type`, etc.
- Size: Approx 1.5GB

---

## üî• Key Tasks & Features

‚úÖ Load Parquet data into Spark DataFrame  
‚úÖ Data exploration and schema inspection  
‚úÖ Data partitioning by **Year** and **Month**  
‚úÖ Repartitioning and caching strategies  
‚úÖ UDF and Broadcast variables for Payment Type mapping  
‚úÖ Total revenue aggregation by Month  
‚úÖ Peak trip hours per Month using Window functions  
‚úÖ Revenue by Payment Type  
‚úÖ Trip Duration calculation  
‚úÖ Data cleaning: dropping duplicates and nulls  
‚úÖ Handling anomalous geo-coordinates (zero values)  
‚úÖ Revenue generated by Vendor  
‚úÖ Toll charge and tax calculations per Year  
‚úÖ Distance classification (Short, Medium, Long)  
‚úÖ Average fare per passenger  
‚úÖ Most popular pickup and dropoff locations  
‚úÖ Correlation between trip distance and fare amount  

---

## üìà Example Insight

> üìä Correlation between **Trip Distance** and **Fare Amount**:  
> Calculated using:  
> ```python
> par_df.stat.corr("trip_distance", "fare_amount")
> ```

---

## üöÄ How to Run This Project

1. Download the dataset using the [source link](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
2. Place the `yellow_tripdata_2016-03.parquet` file inside your local `source/` folder
3. Open `NYC_Taxi_Trip_Analysis.dbc` in **Databricks Community Edition** or your Databricks workspace
4. Attach a running cluster
5. Run each cell sequentially to explore and analyze the data

---

## üìö Technologies Used

- **Python 3**
- **PySpark**
- **Databricks Community Edition**
- **Spark SQL**
- **Parquet**

---

## üì¨ Connect With Me

[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/RAHULMAC05)  
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://linkedin.com/in/rahulpaul52750)

---

## üìå Note
This is a practice data engineering & EDA project built for learning purposes on Databricks Community Edition.

This repository intentionally excludes large dataset files (>100MB) to comply with GitHub's file size limits.  
Please use the download link provided above to obtain the source data.

---

## ‚≠êÔ∏è Give this project a star if you found it useful!
